{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "train.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "0nPBMwUz-8HX"
      },
      "source": [
        "!pip install tf-nightly"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1WLmDj_yWzMv"
      },
      "source": [
        "import pathlib\n",
        "\n",
        "if \"models\" in pathlib.Path.cwd().parts:\n",
        "  while \"models\" in pathlib.Path.cwd().parts:\n",
        "    os.chdir('..')\n",
        "elif not pathlib.Path('models').exists():\n",
        "  !git clone --depth 1 https://github.com/tensorflow/models"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eNnYacNB_3oh"
      },
      "source": [
        "%%bash\n",
        "cd models/research/\n",
        "protoc object_detection/protos/*.proto --python_out=.\n",
        "cp object_detection/packages/tf2/setup.py .\n",
        "python -m pip install ."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EE5RhAQiKt83"
      },
      "source": [
        "!unzip -q ducky.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mhw1-yj5kU6O"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import utils\n",
        "from PIL import Image\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from object_detection.utils import visualization_utils as viz_utils\n",
        "from object_detection.utils import config_util\n",
        "from object_detection.builders import model_builder\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hRkkKtvPNfK1"
      },
      "source": [
        "tensor_images, tensor_boxes, tensor_classes_one_hot, classes_map = utils.load_data('ducky/train', 'ducky/classes.txt')\n",
        "num_classes = len(classes_map)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JYqHcBWjN4lg"
      },
      "source": [
        "print(f\"classes_map = {classes_map}\")\n",
        "\n",
        "category_index = {}\n",
        "for label_name, label_id in classes_map.items():\n",
        "    category_index[label_id] = {\n",
        "        'id': label_id,\n",
        "        'name': label_name\n",
        "    }\n",
        "print(f\"category_index = {category_index}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2TYAhwVkolE"
      },
      "source": [
        "plt.figure(figsize=(8, 6))\n",
        "image = np.squeeze(tensor_images[0].numpy()).astype(np.uint8)\n",
        "plt.imshow(image)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h9pIEqu5CrIH"
      },
      "source": [
        "image_classes = tensor_classes_one_hot[0].numpy().argmax(axis=1) + 1\n",
        "viz_utils.visualize_boxes_and_labels_on_image_array(\n",
        "    image=image,\n",
        "    boxes=tensor_boxes[0].numpy(),\n",
        "    classes=image_classes,\n",
        "    scores=np.ones(image_classes.shape),\n",
        "    category_index=category_index,\n",
        "    use_normalized_coordinates=True,\n",
        "    min_score_thresh=0.8\n",
        ")\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.imshow(image)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m0HXnYawk8GZ"
      },
      "source": [
        "# Download the checkpoint and put it into models/research/object_detection/test_data/\n",
        "\n",
        "!wget http://download.tensorflow.org/models/object_detection/tf2/20200711/ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8.tar.gz\n",
        "!tar -xf ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8.tar.gz\n",
        "!if [ -d \"models/research/object_detection/test_data/checkpoint\" ]; then rm -Rf models/research/object_detection/test_data/checkpoint; fi\n",
        "!mkdir models/research/object_detection/test_data/checkpoint\n",
        "!mv ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8/checkpoint models/research/object_detection/test_data/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZGtI8Ey_lbmL"
      },
      "source": [
        "tf.keras.backend.clear_session()\n",
        "\n",
        "print('Building model and restoring weights for fine-tuning...', flush=True)\n",
        "# num_classes = 1\n",
        "pipeline_config = 'models/research/object_detection/configs/tf2/ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8.config'\n",
        "checkpoint_path = 'models/research/object_detection/test_data/checkpoint/ckpt-0'\n",
        "\n",
        "# This will be where we save checkpoint & config for TFLite conversion later.\n",
        "output_directory = 'output/'\n",
        "output_checkpoint_dir = os.path.join(output_directory, 'checkpoint')\n",
        "\n",
        "# Load pipeline config and build a detection model.\n",
        "#\n",
        "# Since we are working off of a COCO architecture which predicts 90\n",
        "# class slots by default, we override the `num_classes` field here to be just\n",
        "# one (for our new rubber ducky class).\n",
        "configs = config_util.get_configs_from_pipeline_file(pipeline_config)\n",
        "model_config = configs['model']\n",
        "model_config.ssd.num_classes = num_classes\n",
        "model_config.ssd.freeze_batchnorm = True\n",
        "detection_model = model_builder.build(\n",
        "      model_config=model_config, is_training=True)\n",
        "# Save new pipeline config\n",
        "pipeline_proto = config_util.create_pipeline_proto_from_configs(configs)\n",
        "config_util.save_pipeline_config(pipeline_proto, output_directory)\n",
        "\n",
        "# Set up object-based checkpoint restore --- SSD has two prediction\n",
        "# `heads` --- one for classification, the other for box regression.  We will\n",
        "# restore the box regression head but initialize the classification head\n",
        "# from scratch (we show the omission below by commenting out the line that\n",
        "# we would add if we wanted to restore both heads)\n",
        "fake_box_predictor = tf.compat.v2.train.Checkpoint(\n",
        "    _base_tower_layers_for_heads=detection_model._box_predictor._base_tower_layers_for_heads,\n",
        "    # _prediction_heads=detection_model._box_predictor._prediction_heads,\n",
        "    #    (i.e., the classification head that we *will not* restore)\n",
        "    _box_prediction_head=detection_model._box_predictor._box_prediction_head,\n",
        "    )\n",
        "fake_model = tf.compat.v2.train.Checkpoint(\n",
        "          _feature_extractor=detection_model._feature_extractor,\n",
        "          _box_predictor=fake_box_predictor)\n",
        "ckpt = tf.compat.v2.train.Checkpoint(model=fake_model)\n",
        "ckpt.restore(checkpoint_path).expect_partial()\n",
        "\n",
        "# To save checkpoint for TFLite conversion.\n",
        "exported_ckpt = tf.compat.v2.train.Checkpoint(model=detection_model)\n",
        "ckpt_manager = tf.train.CheckpointManager(\n",
        "    exported_ckpt, output_checkpoint_dir, max_to_keep=1)\n",
        "\n",
        "# Run model through a dummy image so that variables are created\n",
        "image, shapes = detection_model.preprocess(tf.zeros([1, 320, 320, 3]))\n",
        "prediction_dict = detection_model.predict(image, shapes)\n",
        "_ = detection_model.postprocess(prediction_dict, shapes)\n",
        "print('Weights restored!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NqGK7JZ6Rs4n"
      },
      "source": [
        "tensor_boxes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ayz8h2pWlrOJ"
      },
      "source": [
        "tf.keras.backend.set_learning_phase(True)\n",
        "\n",
        "# These parameters can be tuned; since our training set has 5 images\n",
        "# it doesn't make sense to have a much larger batch size, though we could\n",
        "# fit more examples in memory if we wanted to.\n",
        "batch_size = 32\n",
        "learning_rate = 0.1\n",
        "num_batches = 1000\n",
        "\n",
        "# Select variables in top layers to fine-tune.\n",
        "trainable_variables = detection_model.trainable_variables\n",
        "to_fine_tune = []\n",
        "prefixes_to_train = [\n",
        "  'WeightSharedConvolutionalBoxPredictor/WeightSharedConvolutionalBoxHead',\n",
        "  'WeightSharedConvolutionalBoxPredictor/WeightSharedConvolutionalClassHead']\n",
        "for var in trainable_variables:\n",
        "  if any([var.name.startswith(prefix) for prefix in prefixes_to_train]):\n",
        "    to_fine_tune.append(var)\n",
        "\n",
        "# Set up forward + backward pass for a single train step.\n",
        "def get_model_train_step_function(model, optimizer, vars_to_fine_tune):\n",
        "  \"\"\"Get a tf.function for training step.\"\"\"\n",
        "\n",
        "  # Use tf.function for a bit of speed.\n",
        "  # Comment out the tf.function decorator if you want the inside of the\n",
        "  # function to run eagerly.\n",
        "  @tf.function(experimental_relax_shapes=True)\n",
        "  def train_step_fn(image_tensors,\n",
        "                    groundtruth_boxes_list,\n",
        "                    groundtruth_classes_list):\n",
        "    \"\"\"A single training iteration.\n",
        "\n",
        "    Args:\n",
        "      image_tensors: A list of [1, height, width, 3] Tensor of type tf.float32.\n",
        "        Note that the height and width can vary across images, as they are\n",
        "        reshaped within this function to be 320x320.\n",
        "      groundtruth_boxes_list: A list of Tensors of shape [N_i, 4] with type\n",
        "        tf.float32 representing groundtruth boxes for each image in the batch.\n",
        "      groundtruth_classes_list: A list of Tensors of shape [N_i, num_classes]\n",
        "        with type tf.float32 representing groundtruth boxes for each image in\n",
        "        the batch.\n",
        "\n",
        "    Returns:\n",
        "      A scalar tensor representing the total loss for the input batch.\n",
        "    \"\"\"\n",
        "    shapes = tf.constant(batch_size * [[320, 320, 3]], dtype=tf.int32)\n",
        "    model.provide_groundtruth(\n",
        "        groundtruth_boxes_list=groundtruth_boxes_list,\n",
        "        groundtruth_classes_list=groundtruth_classes_list)\n",
        "    with tf.GradientTape() as tape:\n",
        "      preprocessed_images = tf.concat(\n",
        "          [detection_model.preprocess(image_tensor)[0]\n",
        "           for image_tensor in image_tensors], axis=0)\n",
        "      prediction_dict = model.predict(preprocessed_images, shapes)\n",
        "      losses_dict = model.loss(prediction_dict, shapes)\n",
        "      total_loss = losses_dict['Loss/localization_loss'] + losses_dict['Loss/classification_loss']\n",
        "      gradients = tape.gradient(total_loss, vars_to_fine_tune)\n",
        "      optimizer.apply_gradients(zip(gradients, vars_to_fine_tune))\n",
        "    return total_loss\n",
        "\n",
        "  return train_step_fn\n",
        "\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate, momentum=0.9)\n",
        "train_step_fn = get_model_train_step_function(\n",
        "    detection_model, optimizer, to_fine_tune)\n",
        "\n",
        "print('Start fine-tuning!', flush=True)\n",
        "for idx in range(num_batches):\n",
        "  # Grab keys for a random subset of examples\n",
        "  all_keys = list(range(len(tensor_images)))\n",
        "  random.shuffle(all_keys)\n",
        "  example_keys = all_keys[:batch_size]\n",
        "\n",
        "  # Note that we do not do data augmentation in this demo.  If you want a\n",
        "  # a fun exercise, we recommend experimenting with random horizontal flipping\n",
        "  # and random cropping :)\n",
        "  gt_boxes_list = [tensor_boxes[key] for key in example_keys]\n",
        "  gt_classes_list = [tensor_classes_one_hot[key] for key in example_keys]\n",
        "  image_tensors = [tensor_images[key] for key in example_keys]\n",
        "\n",
        "  # Training step (forward pass + backwards pass)\n",
        "  total_loss = train_step_fn(image_tensors, gt_boxes_list, gt_classes_list)\n",
        "\n",
        "  if idx % 100 == 0:\n",
        "    print('batch ' + str(idx) + ' of ' + str(num_batches)\n",
        "    + ', loss=' +  str(total_loss.numpy()), flush=True)\n",
        "\n",
        "print('Done fine-tuning!')\n",
        "\n",
        "ckpt_manager.save()\n",
        "print('Checkpoint saved!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l9hbpPbulv8M"
      },
      "source": [
        "%%bash\n",
        "python models/research/object_detection/export_tflite_graph_tf2.py \\\n",
        "  --pipeline_config_path output/pipeline.config \\\n",
        "  --trained_checkpoint_dir output/checkpoint \\\n",
        "  --output_directory tflite"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MLEcdI7ImEmN"
      },
      "source": [
        "!tflite_convert --saved_model_dir=tflite/saved_model --output_file=tflite/model.tflite"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rqge2bVQmSxY"
      },
      "source": [
        "def detect(interpreter, input_tensor):\n",
        "    input_details = interpreter.get_input_details()\n",
        "    output_details = interpreter.get_output_details()\n",
        "\n",
        "    preprocessed_image, shapes = detection_model.preprocess(input_tensor)\n",
        "    interpreter.set_tensor(input_details[0]['index'], preprocessed_image.numpy())\n",
        "\n",
        "    interpreter.invoke()\n",
        "\n",
        "    boxes = interpreter.get_tensor(output_details[0]['index'])\n",
        "    classes = interpreter.get_tensor(output_details[1]['index'])\n",
        "    scores = interpreter.get_tensor(output_details[2]['index'])\n",
        "\n",
        "    return boxes, classes, scores"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZcSEJQeWb9rX"
      },
      "source": [
        "interpreter = tf.lite.Interpreter(model_path=\"tflite/model.tflite\")\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "input_details = interpreter.get_input_details()\n",
        "print(f\"input shape: {input_details[0]['shape']}\")\n",
        "output_details = interpreter.get_output_details()\n",
        "print(f\"output boxes: {output_details[0]['shape']}\")\n",
        "print(f\"output classes: {output_details[1]['shape']}\")\n",
        "print(f\"output scores: {output_details[2]['shape']}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f_M_iPnObCFL"
      },
      "source": [
        "test_image = Image.open('./ducky/test/out1.jpg')\n",
        "test_image = np.array(test_image)\n",
        "tensor_test_image = tf.expand_dims(tf.convert_to_tensor(test_image, dtype=tf.float32), axis=0)\n",
        "\n",
        "boxes, classes, scores = detect(interpreter, tensor_test_image)\n",
        "\n",
        "viz_utils.visualize_boxes_and_labels_on_image_array(\n",
        "    image=test_image,\n",
        "    boxes=boxes[0],\n",
        "    classes=classes[0].astype(np.uint32) + utils.LABEL_ID_OFFSET,\n",
        "    scores=scores[0],\n",
        "    category_index=category_index,\n",
        "    use_normalized_coordinates=True,\n",
        "    min_score_thresh=0.2\n",
        ")\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.imshow(test_image)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}